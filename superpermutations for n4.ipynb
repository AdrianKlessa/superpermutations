{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finding short superpermutations for n=4",
   "id": "44577e4bb392f49d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T11:35:03.163625Z",
     "start_time": "2024-12-30T11:34:59.407087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from GymPermutationsEnv import GymPermutationEnv\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(GymPermutationEnv, n_envs=16, env_kwargs=dict(alphabet_size=4), vec_env_cls=SubprocVecEnv)"
   ],
   "id": "9eec6592a776cba4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T11:44:23.599588Z",
     "start_time": "2024-12-30T11:35:03.163625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the agent\n",
    "#model = A2C(\"MlpPolicy\", vec_env, verbose=1,policy_kwargs=dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))).learn(total_timesteps=100000)\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "eval_env = make_vec_env(GymPermutationEnv, env_kwargs=dict(alphabet_size=4), vec_env_cls=SubprocVecEnv)\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=63, verbose=1) # for why 63 see notes below about calculating the max possible reward\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
    "model = PPO('MlpPolicy', vec_env, verbose=1)\n",
    "model.learn(10000000, callback=eval_callback)"
   ],
   "id": "ae45cbcc4c940b00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    fps             | 8447     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 56.9       |\n",
      "|    ep_rew_mean          | -162       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2859       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01294235 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.17      |\n",
      "|    explained_variance   | 0.00199    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 156        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 708        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50.1        |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2329        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009772977 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.15       |\n",
      "|    explained_variance   | 0.0727      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 420         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 812         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 43.1        |\n",
      "|    ep_rew_mean          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2112        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012319913 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.12       |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 408         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 868         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=44.00 +/- 0.00\n",
      "Episode length: 16.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 44          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015770433 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.07       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 333         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 771         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.1     |\n",
      "|    ep_rew_mean     | -57.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 2014     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | -21.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1948        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019762326 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.99       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 207         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 566         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.1        |\n",
      "|    ep_rew_mean          | -2.97       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1898        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024102315 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 174         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 334         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.6        |\n",
      "|    ep_rew_mean          | 19.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1858        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025723128 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.00856     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.7        |\n",
      "|    ep_rew_mean          | 25.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1837        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022535378 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | -0.0302     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=54.00 +/- 0.00\n",
      "Episode length: 14.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 54          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021798529 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 59.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.3     |\n",
      "|    ep_rew_mean     | 30.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1827     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 17.3       |\n",
      "|    ep_rew_mean          | 35.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1818       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 198        |\n",
      "|    total_timesteps      | 360448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02289475 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.37      |\n",
      "|    explained_variance   | 0.623      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20         |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    value_loss           | 42.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.1        |\n",
      "|    ep_rew_mean          | 42          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1808        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025756788 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 31.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.7        |\n",
      "|    ep_rew_mean          | 45          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1801        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025574274 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.7        |\n",
      "|    ep_rew_mean          | 48.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1795        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026206087 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 18          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=60.00 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026279472 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.39        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 14.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.5     |\n",
      "|    ep_rew_mean     | 50.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 1792     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.8        |\n",
      "|    ep_rew_mean          | 52.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1790        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027830185 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.53        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.4        |\n",
      "|    ep_rew_mean          | 54.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1786        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028797517 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 9.93        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13          |\n",
      "|    ep_rew_mean          | 56.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1779        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029919824 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.34        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 8.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.7        |\n",
      "|    ep_rew_mean          | 58.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1774        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030829016 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.9         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 6.31        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=62.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10         |\n",
      "|    mean_reward          | 62         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 640000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03408254 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.92       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0263    |\n",
      "|    value_loss           | 4.79       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.1     |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 1772     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 11.3     |\n",
      "|    ep_rew_mean          | 60.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 1769     |\n",
      "|    iterations           | 21       |\n",
      "|    time_elapsed         | 388      |\n",
      "|    total_timesteps      | 688128   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.033429 |\n",
      "|    clip_fraction        | 0.315    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.981   |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.08     |\n",
      "|    n_updates            | 200      |\n",
      "|    policy_gradient_loss | -0.0249  |\n",
      "|    value_loss           | 3.13     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.9        |\n",
      "|    ep_rew_mean          | 61.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1766        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 720896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048784867 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.4       |\n",
      "|    ep_rew_mean          | 62.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1764       |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 427        |\n",
      "|    total_timesteps      | 753664     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06616138 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.558     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.883      |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    value_loss           | 1.12       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.4        |\n",
      "|    ep_rew_mean          | 62.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1761        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037797876 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=57.00 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 57          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089337826 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.385       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.3     |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 1757     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 466      |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.5       |\n",
      "|    ep_rew_mean          | 61.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1755       |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 485        |\n",
      "|    total_timesteps      | 851968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06534156 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.567     |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.6       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    value_loss           | 21.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.3        |\n",
      "|    ep_rew_mean          | 58.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1754        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055970214 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 7.56        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.7       |\n",
      "|    ep_rew_mean          | 61.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1753       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 523        |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06998357 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.553     |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.1       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    value_loss           | 34.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.4       |\n",
      "|    ep_rew_mean          | 62.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1752       |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 542        |\n",
      "|    total_timesteps      | 950272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08533232 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.436     |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.45       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 4.02       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=63.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 63          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061853852 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 63.00  is above the threshold 63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2e3113654d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T11:44:23.617790Z",
     "start_time": "2024-12-30T11:44:23.599588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the trained agent\n",
    "vec_env = make_vec_env(GymPermutationEnv, n_envs=1, env_kwargs=dict(alphabet_size=4))\n",
    "obs = vec_env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ],
   "id": "85659ed1af3545d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  [3]\n",
      "obs= [[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]] reward= [0.] done= [False]\n",
      "[1, 3, 4, 2]\n",
      "Step 2\n",
      "Action:  [6]\n",
      "obs= [[0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]] reward= [9.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4]\n",
      "Step 3\n",
      "Action:  [18]\n",
      "obs= [[0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0]] reward= [5.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3]\n",
      "Step 4\n",
      "Action:  [19]\n",
      "obs= [[1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0]] reward= [8.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2]\n",
      "Step 5\n",
      "Action:  [10]\n",
      "obs= [[1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]] reward= [9.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3]\n",
      "Step 6\n",
      "Action:  [13]\n",
      "obs= [[1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0]] reward= [1.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2]\n",
      "Step 7\n",
      "Action:  [8]\n",
      "obs= [[1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]] reward= [9.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4]\n",
      "Step 8\n",
      "Action:  [14]\n",
      "obs= [[1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0]] reward= [8.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4, 3, 2, 1, 4]\n",
      "Step 9\n",
      "Action:  [12]\n",
      "obs= [[1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0]] reward= [8.] done= [False]\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4, 3, 2, 1, 4, 3, 1, 2, 4]\n",
      "Step 10\n",
      "Action:  [11]\n",
      "Below/at upper bound found for n=4:\n",
      "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4, 3, 2, 1, 4, 3, 1, 2, 4, 3, 1]\n",
      "obs= [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]] reward= [6.] done= [ True]\n",
      "[]\n",
      "Goal reached! reward= [6.]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The RL agent found the following superpermutation:\n",
    "[1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4, 3, 2, 1, 4, 3, 1, 2, 4, 3, 1]\n",
    "\n",
    "We can check if it's the same (up to relabelling) as the shortest superpermutation shown on Wikipedia:"
   ],
   "id": "3b6d93f90c007b2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T11:46:42.693967Z",
     "start_time": "2024-12-30T11:46:42.688757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import permutation_utils\n",
    "found = [1, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 3, 1, 4, 2, 3, 1, 4, 3, 2, 1, 4, 3, 1, 2, 4, 3, 1]\n",
    "shortest_superpermutation = [int(i) for i in \"123412314231243121342132413214321\"]\n",
    "relabellings_list = permutation_utils.get_possible_relabellings(shortest_superpermutation, [1,2,3,4])\n",
    "print(found in relabellings_list)"
   ],
   "id": "bc062972a7e6e1ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Calculating the maximum possible reward for an episode\n",
    "\n",
    "Assume that n is the size of the alphabet. Let us call the superpermutation formed by appending every possible permutation a \"naive superpermutation\".\n",
    "\n",
    "Since there are n! possible permutations, each having length n and every one has to be included, the naive superpermutation has length n!*n.\n",
    "\n",
    "The rewards in the environment are formed in such a way that the model gets 1 point for each character \"saved\" compared to this naive superpermutation. That is, if merging two permutations that have 2 overlapping characters, we get 2 points. If by merging we also added another permutation (other than the two we merged), we additionally reward the model with n points.\n",
    "\n",
    "Thus the cumulative reward for an episode is (aside from penalties for picking already added permutations) equal to (n!*n)-(length of the superpermutation created by the model).\n",
    "\n",
    "So the length of the superpermutation created by the model is at most (n!*n)-(sum of cumulative rewards for the episode)\n",
    "\n",
    "For n=4, this is (4!*4)-(sum of cumulative rewards for the episode).\n",
    "\n",
    "33 = 96-reward-->reward=63"
   ],
   "id": "1421099d78f83d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de1e39f56433cfa3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
