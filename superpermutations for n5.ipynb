{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finding short superpermutations for n=5",
   "id": "6ee96f072b92992a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:59:39.196358Z",
     "start_time": "2025-06-18T10:59:23.188492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from GymPermutationsEnv import GymPermutationEnv"
   ],
   "id": "101919a42f0bfc35",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:59:45.338491Z",
     "start_time": "2025-06-18T10:59:45.334968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import permutation_utils\n",
    "\n",
    "alphabet_size = 5\n",
    "\n",
    "max_reward_unscaled = permutation_utils.get_max_possible_reward(5, 153)\n",
    "max_reward_scaled = max_reward_unscaled/alphabet_size"
   ],
   "id": "7c1aec3f4c829386",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:59:45.370615Z",
     "start_time": "2025-06-18T10:59:45.365752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "math.factorial(alphabet_size)*alphabet_size"
   ],
   "id": "1873d76c90b9d298",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T10:59:50.868026Z",
     "start_time": "2025-06-18T10:59:45.387666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=None,\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"
   ],
   "id": "d1e08f67d1849458",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T11:03:30.143504Z",
     "start_time": "2025-06-18T10:59:51.586693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the env\n",
    "vec_env = make_vec_env(GymPermutationEnv, n_envs=16, env_kwargs=dict(alphabet_size=alphabet_size), vec_env_cls=SubprocVecEnv)\n",
    "eval_env = make_vec_env(GymPermutationEnv, env_kwargs=dict(alphabet_size=alphabet_size), vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "# Set up hyperparameters\n",
    "# More conservative settings to counter policy collapse\n",
    "hp_policy_type = \"MlpPolicy\"\n",
    "hp_learning_rate = 5e-5\n",
    "hp_clip_range = 0.1\n",
    "hp_batch_size = 128\n",
    "hp_n_steps=4096\n",
    "hp_seed = 42 # Not really a hyperparameter, unless we're extremely unlucky...\n",
    "hp_training_timesteps = 2e8\n",
    "\n",
    "# Train the agent\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=max_reward_scaled, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"alphabet_size\", alphabet_size)\n",
    "\n",
    "    mlflow.log_param(\"policy_type\", hp_policy_type)\n",
    "    mlflow.log_param(\"learning_rate\", hp_learning_rate)\n",
    "    mlflow.log_param(\"clip_range\", hp_clip_range)\n",
    "    mlflow.log_param(\"batch_size\", hp_batch_size)\n",
    "    mlflow.log_param(\"n_steps\", hp_n_steps)\n",
    "    mlflow.log_param(\"seed\", hp_seed)\n",
    "    mlflow.log_param(\"training_timesteps\", hp_training_timesteps)\n",
    "\n",
    "    model = PPO(hp_policy_type,\n",
    "                vec_env,\n",
    "                verbose=1,\n",
    "                batch_size=hp_batch_size,\n",
    "                clip_range=hp_clip_range,\n",
    "                seed=hp_seed,\n",
    "                n_steps=hp_n_steps,\n",
    "                learning_rate=hp_learning_rate)\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(int(hp_training_timesteps), callback=eval_callback)\n"
   ],
   "id": "d1cc6314d1adcaf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 154      |\n",
      "|    ep_rew_mean     | -48.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 1985     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | -48.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1256         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024733394 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -4.79        |\n",
      "|    explained_variance   | -0.0165      |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.47         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 7.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-49.80 +/- 4.92\n",
      "Episode length: 154.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 154          |\n",
      "|    mean_reward          | -49.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019656604 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -4.78        |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 2.23         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 6.81         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 154      |\n",
      "|    ep_rew_mean     | -47.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 1052     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "🏃 View run victorious-sow-3 at: http://127.0.0.1:8080/#/experiments/0/runs/80606631dd864f6dbdea3e0e24f90c9b\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 43\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Set custom logger\u001B[39;00m\n\u001B[0;32m     42\u001B[0m model\u001B[38;5;241m.\u001B[39mset_logger(loggers)\n\u001B[1;32m---> 43\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhp_training_timesteps\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_callback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    304\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    309\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    310\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:336\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mep_info_buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    334\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dump_logs(iteration)\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:213\u001B[0m, in \u001B[0;36mPPO.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, spaces\u001B[38;5;241m.\u001B[39mDiscrete):\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;66;03m# Convert discrete action from float to long\u001B[39;00m\n\u001B[0;32m    211\u001B[0m     actions \u001B[38;5;241m=\u001B[39m rollout_data\u001B[38;5;241m.\u001B[39mactions\u001B[38;5;241m.\u001B[39mlong()\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m--> 213\u001B[0m values, log_prob, entropy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrollout_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    215\u001B[0m \u001B[38;5;66;03m# Normalize advantage\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:730\u001B[0m, in \u001B[0;36mActorCriticPolicy.evaluate_actions\u001B[1;34m(self, obs, actions)\u001B[0m\n\u001B[0;32m    720\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;124;03mEvaluate actions according to the current policy,\u001B[39;00m\n\u001B[0;32m    722\u001B[0m \u001B[38;5;124;03mgiven the observations.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;124;03m    and entropy of the action distribution.\u001B[39;00m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    729\u001B[0m \u001B[38;5;66;03m# Preprocess the observation if needed\u001B[39;00m\n\u001B[1;32m--> 730\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[0;32m    732\u001B[0m     latent_pi, latent_vf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor(features)\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001B[0m, in \u001B[0;36mActorCriticPolicy.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;124;03mPreprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    665\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;124;03m    features for the actor and the features for the critic.\u001B[39;00m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[1;32m--> 672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m features_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:130\u001B[0m, in \u001B[0;36mBaseModel.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    123\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m    Preprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m    :return: The extracted features\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 130\u001B[0m     preprocessed_obs \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_obs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize_images\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001B[1;32mE:\\Dev\\PyCharm projects\\superpermutations\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:129\u001B[0m, in \u001B[0;36mpreprocess_obs\u001B[1;34m(obs, observation_space, normalize_images)\u001B[0m\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mone_hot(obs\u001B[38;5;241m.\u001B[39mlong(), num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(observation_space\u001B[38;5;241m.\u001B[39mn))\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation_space, spaces\u001B[38;5;241m.\u001B[39mMultiDiscrete):\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;66;03m# Tensor concatenation of one hot encodings of each Categorical sub-space\u001B[39;00m\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_hot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnvec\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobs_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(obs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28msum\u001B[39m(observation_space\u001B[38;5;241m.\u001B[39mnvec))\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation_space, spaces\u001B[38;5;241m.\u001B[39mMultiBinary):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obs\u001B[38;5;241m.\u001B[39mfloat()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the trained agent\n",
    "vec_env = make_vec_env(GymPermutationEnv, n_envs=1, env_kwargs=dict(alphabet_size=alphabet_size))\n",
    "obs = vec_env.reset()\n",
    "n_steps = math.factorial(alphabet_size)\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ],
   "id": "7b8c9fcac7c03748"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
